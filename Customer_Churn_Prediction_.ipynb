{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80ecc2be",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction\n",
    "\n",
    "This notebook demonstrates a complete workflow for predicting customer churn using a dataset of Netflix customers.\n",
    "We will cover:\n",
    "1. Data Loading and Exploration\n",
    "2. Data Cleaning and Preprocessing\n",
    "3. Feature Engineering\n",
    "4. Model Training and Hyperparameter Tuning\n",
    "5. Model Stacking and Final Evaluation\n",
    "6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef434a73",
   "metadata": {
    "id": "y2o_mHqPg9sT",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Handling imbalance\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Metrics / Evaluation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2502760",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ANX8rjbus2w",
    "outputId": "ae038d24-2c20-475b-baea-ffa5ead7c8f1"
   },
   "outputs": [],
   "source": [
    "!pip install ace_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49132dd1",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration\n",
    "Import necessary libraries for data manipulation, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3662fce2",
   "metadata": {
    "id": "J1eHZg_6vFhs",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from time import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "\n",
    "# optional imports (will continue gracefully if absent)\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except Exception:\n",
    "    CATBOOST_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "except Exception:\n",
    "    LGB_AVAILABLE = False\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca114f",
   "metadata": {
    "id": "9zITXP3Jvfuq"
   },
   "source": [
    "## **Config**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d76ee9e",
   "metadata": {
    "id": "9zITXP3Jvfuq"
   },
   "source": [
    "## **Config**\n",
    "Define paths and constants used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7241c72",
   "metadata": {
    "id": "EMd5TturvPgu"
   },
   "outputs": [],
   "source": [
    "CSV_PATH = \"netflix_customer_churn.csv\"\n",
    "OUTPUT_DIR = \"/content/output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "OUTPUT_MODEL_PATH = \"model.pkl\"\n",
    "OUTPUT_SUMMARY_CSV = os.path.join(OUTPUT_DIR, \"model_comparison_summary.csv\")\n",
    "OUTPUT_TEXT_SUMMARY = os.path.join(OUTPUT_DIR, \"best_model_summary.txt\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_JOBS = -1\n",
    "N_SPLITS = 5\n",
    "TOP_K_FEATURES = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557feb81",
   "metadata": {
    "id": "r8tBJgWiwAda"
   },
   "source": [
    "# **Load** **Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c14a48",
   "metadata": {
    "id": "r8tBJgWiwAda"
   },
   "source": [
    "# **Load Data**\n",
    "Load the dataset and perform an initial inspection of the data structure and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af167fb7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9y39c7ov--s",
    "outputId": "0d3ae0e1-e64f-4313-ee4e-351fa887bdb1"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Loaded:\", CSV_PATH, \"shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f8e5c",
   "metadata": {
    "id": "1iP7ymoYhYaN"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41203c1",
   "metadata": {
    "id": "z8ohw6nFhfBl"
   },
   "outputs": [],
   "source": [
    "df[\"churned\"].value_counts()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40947936",
   "metadata": {
    "id": "txNU3WCchiFx"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651b4ac8",
   "metadata": {
    "id": "HuLyYJhjhkn7"
   },
   "outputs": [],
   "source": [
    "df[\"gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a90b8",
   "metadata": {
    "id": "dQISfNPyhlQs",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df[\"watch_hours\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a24a51",
   "metadata": {
    "id": "Uf6UHLrdhpaZ",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df[\"customer_id\"].duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4ac40f",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Rename columns for consistency and drop unnecessary columns that won't be used for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfc5e29",
   "metadata": {
    "id": "JHfiK5HJhsfK",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"watch_hours\": \"month_watch_hours\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb2350",
   "metadata": {
    "id": "H04NWEV4hx3k",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\n",
    "    \"region\",\n",
    "    \"device\",\n",
    "    \"payment_method\",\n",
    "    \"avg_watch_time_per_day\",\n",
    "    \"favorite_genre\",\n",
    "    \"number_of_profiles\",\n",
    "    \"customer_id\"\n",
    "])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f128b806",
   "metadata": {
    "id": "dgAzLUs4h87J",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68930e26",
   "metadata": {
    "id": "vXv1sRYUh_CR",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "(df== \"\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60313351",
   "metadata": {
    "id": "lyFP7izsiBn0",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350e0303",
   "metadata": {
    "id": "m1L8Em7WiE3J",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df= df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa5c8f1",
   "metadata": {
    "id": "13VlTzeHiH-3"
   },
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2d9a53",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "Visualize the distribution of key variables like Gender and Churn status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9273823e",
   "metadata": {
    "id": "ycc2ICpriKgC",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df[\"churned\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152d3d5b",
   "metadata": {
    "id": "1WEic9JniO4E"
   },
   "outputs": [],
   "source": [
    "df[\"gender\"].value_counts().plot(kind=\"bar\")\n",
    "\n",
    "plt.title(\"Gender Distribution\")\n",
    "plt.xlabel(\"Gender\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6891283",
   "metadata": {
    "id": "rIxcYd1oiMcx",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df[\"churned\"].value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Churn Distribution\")\n",
    "plt.xlabel(\"Churn (0=No, 1=Yes)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef8bde9",
   "metadata": {
    "id": "u6Gx5D-jwMLR"
   },
   "source": [
    "# **Column mapping & selection (as before)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601c2674",
   "metadata": {
    "id": "u6Gx5D-jwMLR"
   },
   "source": [
    "# **Column mapping & selection**\n",
    "Select the relevant columns for modeling and handle any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb2f0de",
   "metadata": {
    "id": "Fxur0lqRwHO6",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "monthly_col = \"watch_hours\" if \"watch_hours\" in df.columns else (\"month_watch_hours\" if \"month_watch_hours\" in df.columns else None)\n",
    "txn_col = \"num_transactions\" if \"num_transactions\" in df.columns else (\"num_transactions_scaled\" if \"num_transactions_scaled\" in df.columns else None)\n",
    "target_col = \"churned\" if \"churned\" in df.columns else (\"churn\" if \"churn\" in df.columns else None)\n",
    "\n",
    "if not (monthly_col and txn_col and target_col):\n",
    "    raise RuntimeError(\"Required columns missing in CSV. Found: \" + \", \".join(df.columns.tolist()))\n",
    "\n",
    "use_cols = [\"age\", \"gender\", monthly_col, txn_col, \"complaints\", \"subscription_type\", \"last_login_days\", target_col]\n",
    "df_sel = df[use_cols].copy()\n",
    "df_sel = df_sel.rename(columns={monthly_col: \"monthly_usage_hours\", txn_col: \"num_transactions\", target_col: \"churn\"})\n",
    "\n",
    "# drop NA rows in these selected cols\n",
    "df_sel = df_sel.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109db91d",
   "metadata": {
    "id": "K_najz5cwSdH"
   },
   "source": [
    "# Feature engineering (important additions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eb40f4",
   "metadata": {
    "id": "K_najz5cwSdH"
   },
   "source": [
    "# Feature Engineering\n",
    "Create new features to capture more complex patterns in the data, such as usage intensity and complaint ratios.\n",
    "We also apply log transformations to skewed variables and bin age into categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676713c4",
   "metadata": {
    "id": "6NOwa1wmwLCc"
   },
   "outputs": [],
   "source": [
    "# usage intensity (hours per transaction) — handles division by zero\n",
    "df_sel[\"usage_intensity\"] = df_sel[\"monthly_usage_hours\"] / (df_sel[\"num_transactions\"] + 1e-6)\n",
    "\n",
    "# complaint ratio\n",
    "df_sel[\"complaint_ratio\"] = df_sel[\"complaints\"] / (df_sel[\"num_transactions\"] + 1e-6)\n",
    "\n",
    "# log transforms to reduce skew\n",
    "df_sel[\"log_monthly_usage\"] = np.log1p(df_sel[\"monthly_usage_hours\"])\n",
    "df_sel[\"log_num_transactions\"] = np.log1p(df_sel[\"num_transactions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d7f7d",
   "metadata": {
    "id": "KfBaM4_ZweLN"
   },
   "outputs": [],
   "source": [
    "# age bucket\n",
    "def age_bucket(a):\n",
    "    if a <= 30:\n",
    "        return \"Young\"\n",
    "    elif a <= 50:\n",
    "        return \"Adult\"\n",
    "    else:\n",
    "        return \"Senior\"\n",
    "df_sel[\"age_bucket\"] = df_sel[\"age\"].apply(age_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a226059f",
   "metadata": {
    "id": "O1FeQ2WswiUg"
   },
   "outputs": [],
   "source": [
    "# Clip winsorize numeric extremes (1st-99th)\n",
    "for col in [\"monthly_usage_hours\", \"num_transactions\", \"usage_intensity\", \"complaint_ratio\", \"log_monthly_usage\", \"log_num_transactions\"]:\n",
    "    low, high = df_sel[col].quantile(0.01), df_sel[col].quantile(0.99)\n",
    "    df_sel[col] = df_sel[col].clip(low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45303ea9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BVApq8ihwlT-",
    "lines_to_next_cell": 2,
    "outputId": "3e4bc47c-6b25-41fe-e027-26e7e288748c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ensure types\n",
    "df_sel[\"age\"] = df_sel[\"age\"].astype(int)\n",
    "df_sel[\"num_transactions\"] = df_sel[\"num_transactions\"].astype(int)\n",
    "df_sel[\"complaints\"] = df_sel[\"complaints\"].astype(int)\n",
    "df_sel[\"churn\"] = df_sel[\"churn\"].astype(int)\n",
    "df_sel[\"gender\"] = df_sel[\"gender\"].astype(str)\n",
    "df_sel[\"subscription_type\"] = df_sel[\"subscription_type\"].astype(str)\n",
    "\n",
    "print(\"After FE, columns:\", df_sel.columns.tolist())\n",
    "print(\"Churn distribution:\\n\", df_sel[\"churn\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0789a7f",
   "metadata": {
    "id": "ThgtnvHlwrIK"
   },
   "source": [
    "# Prepare X,y and split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb3c083",
   "metadata": {
    "id": "ThgtnvHlwrIK"
   },
   "source": [
    "# Prepare X, y and Split\n",
    "Separate the target variable from the features and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf12089",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7tGQ-GzwpFp",
    "lines_to_next_cell": 2,
    "outputId": "51c86bb6-cebe-4e38-b90a-5ed6497a5c0c"
   },
   "outputs": [],
   "source": [
    "target = \"churn\"\n",
    "X = df_sel.drop(columns=[target])\n",
    "y = df_sel[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "print(\"Train/test shapes:\", X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574404e0",
   "metadata": {
    "id": "S0wgEi7mw2LZ"
   },
   "source": [
    "# **Preprocessing definition**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d25d75",
   "metadata": {
    "id": "S0wgEi7mw2LZ"
   },
   "source": [
    "# **Preprocessing Pipeline**\n",
    "Define a preprocessing pipeline to handle numeric scaling and categorical encoding automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4823711",
   "metadata": {
    "id": "dSwsh99qwzl0",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "numeric_features = [\"age\", \"monthly_usage_hours\", \"num_transactions\", \"complaints\", \"usage_intensity\", \"complaint_ratio\", \"log_monthly_usage\", \"log_num_transactions\", \"last_login_days\"]\n",
    "categorical_features = [\"gender\", \"subscription_type\", \"age_bucket\"]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47489f63",
   "metadata": {
    "id": "HoI54tepw9Tk"
   },
   "source": [
    "OneHotEncoder with sparse=False for easier feature-name mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fae3a41",
   "metadata": {
    "id": "Awyr1pkMw7LW"
   },
   "outputs": [],
   "source": [
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat\", categorical_transformer, categorical_features)\n",
    "], remainder=\"drop\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef07fa53",
   "metadata": {
    "id": "E8q6DDp-xDCU"
   },
   "outputs": [],
   "source": [
    "# get feature names helper\n",
    "def get_feature_names(preprocessor):\n",
    "    num_feats = numeric_features\n",
    "    cat_transformer = preprocessor.named_transformers_[\"cat\"]\n",
    "    cat_names = list(cat_transformer.get_feature_names_out(categorical_features))\n",
    "    return num_feats + cat_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12ce84d",
   "metadata": {
    "id": "_51i6txZxK0N"
   },
   "source": [
    "# Model candidates (with sensible defaults)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7bb8c",
   "metadata": {
    "id": "_51i6txZxK0N"
   },
   "source": [
    "# Model Selection\n",
    "Define a list of candidate models to evaluate. We include Logistic Regression, Random Forest, Gradient Boosting, and XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a60a43c",
   "metadata": {
    "id": "Fckwk3jyxI21"
   },
   "outputs": [],
   "source": [
    "candidates = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=2000, random_state=RANDOM_STATE),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE, n_jobs=N_JOBS),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(n_estimators=300, random_state=RANDOM_STATE),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=RANDOM_STATE, n_jobs=N_JOBS)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494e797b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0m4Bkn8xQ0v",
    "outputId": "bc8e3925-dba4-4e8a-bf1a-dd26e4c0b479"
   },
   "outputs": [],
   "source": [
    "if CATBOOST_AVAILABLE:\n",
    "    candidates[\"CatBoost\"] = CatBoostClassifier(verbose=0, random_state=RANDOM_STATE)\n",
    "if LGB_AVAILABLE:\n",
    "    candidates[\"LightGBM\"] = lgb.LGBMClassifier(random_state=RANDOM_STATE, n_jobs=N_JOBS)\n",
    "\n",
    "print(\"Candidate estimators:\", list(candidates.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d5f067",
   "metadata": {
    "id": "aUWC7IBNxagj"
   },
   "source": [
    "Evaluate each candidate with Stratified K-Fold CV (ROC-AUC)\n",
    "\n",
    "We'll fit pipeline: preprocessor -> standard scaling -> estimator using sklearn Pipeline is not straightforward with CV scoring.\n",
    "\n",
    "So we'll use a simple approach: prefit preprocessing, then perform standard scaling on training folds inside cross-validation manually.\n",
    "\n",
    "Simpler: use pipeline with standard scaling and cross_val_score on pipeline (sklearn supports cv).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95ea83c",
   "metadata": {
    "id": "aUWC7IBNxagj"
   },
   "source": [
    "## Cross-Validation\n",
    "Evaluate each model using Stratified K-Fold Cross-Validation to ensure robust performance estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987e896d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_BJFlamixUMX",
    "lines_to_next_cell": 2,
    "outputId": "f8c90570-e905-4a20-8857-73cf7affdb7d"
   },
   "outputs": [],
   "source": [
    "\n",
    "cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "model_scores = []\n",
    "\n",
    "for name, estimator in candidates.items():\n",
    "    print(f\"\\nEvaluating {name} ...\")\n",
    "    # pipeline: preprocessor -> SMOTE -> estimator\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"clf\", estimator)\n",
    "    ])\n",
    "    # cross_val_score with ROC-AUC\n",
    "    try:\n",
    "        scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"roc_auc\", n_jobs=N_JOBS)\n",
    "        mean_auc = float(np.mean(scores))\n",
    "        std_auc = float(np.std(scores))\n",
    "        print(f\"{name} CV ROC-AUC: {mean_auc:.4f} ± {std_auc:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(\"CV failed for\", name, \"->\", e)\n",
    "        mean_auc, std_auc = np.nan, np.nan\n",
    "    model_scores.append({\"model\": name, \"mean_roc_auc\": mean_auc, \"std_roc_auc\": std_auc, \"estimator\": estimator})\n",
    "\n",
    "# Convert to DataFrame\n",
    "scores_df = pd.DataFrame(model_scores).sort_values(\"mean_roc_auc\", ascending=False).reset_index(drop=True)\n",
    "print(\"\\nModel CV ranking:\")\n",
    "print(scores_df[[\"model\",\"mean_roc_auc\",\"std_roc_auc\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ddecaa",
   "metadata": {
    "id": "aAd7Uld1xp7L"
   },
   "source": [
    "**# Quick hyperparameter tuning for top 3 models (RandomizedSearchCV)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d587230b",
   "metadata": {
    "id": "aAd7Uld1xp7L"
   },
   "source": [
    "# Hyperparameter Tuning\n",
    "Select the top performing models and tune their hyperparameters using RandomizedSearchCV to optimize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccdab93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zs2x70BVxm_l",
    "outputId": "cf0df502-9351-4159-f910-0c0031171fc2"
   },
   "outputs": [],
   "source": [
    "top_models = scores_df[\"model\"].tolist()[:3]\n",
    "print(\"\\nTop models for tuning:\", top_models)\n",
    "\n",
    "tuned_estimators = {}\n",
    "random_search_results = []\n",
    "\n",
    "for name in top_models:\n",
    "    base = candidates[name]\n",
    "    print(f\"\\nTuning {name} ...\")\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"clf\", base)\n",
    "    ])\n",
    "    param_distributions = None\n",
    "    n_iter = 20\n",
    "    if name == \"RandomForest\":\n",
    "        param_distributions = {\n",
    "            \"clf__n_estimators\": [200, 300, 500],\n",
    "            \"clf__max_depth\": [6, 10, 16, None],\n",
    "            \"clf__min_samples_split\": [2, 5, 10],\n",
    "            \"clf__max_features\": [\"sqrt\", \"log2\"]\n",
    "        }\n",
    "    elif name == \"XGBoost\":\n",
    "        param_distributions = {\n",
    "            \"clf__n_estimators\": [200, 400, 700],\n",
    "            \"clf__learning_rate\": [0.01, 0.05, 0.1],\n",
    "            \"clf__max_depth\": [3, 5, 7],\n",
    "            \"clf__subsample\": [0.7, 0.8, 1.0],\n",
    "            \"clf__colsample_bytree\": [0.6, 0.8, 1.0]\n",
    "        }\n",
    "    elif name == \"GradientBoosting\":\n",
    "        param_distributions = {\n",
    "            \"clf__n_estimators\": [200, 300, 500],\n",
    "            \"clf__learning_rate\": [0.01, 0.05, 0.1],\n",
    "            \"clf__max_depth\": [3, 5, 7]\n",
    "        }\n",
    "    else:\n",
    "        param_distributions = {\"clf__C\": [0.01, 0.1, 1, 10]}  # fallback for Logistic\n",
    "\n",
    "    # run RandomizedSearchCV\n",
    "    rsearch = RandomizedSearchCV(pipe, param_distributions, n_iter=n_iter, scoring=\"roc_auc\",\n",
    "                                 cv=cv, random_state=RANDOM_STATE, n_jobs=N_JOBS, verbose=0)\n",
    "    try:\n",
    "        t0 = time()\n",
    "        rsearch.fit(X_train, y_train)\n",
    "        t1 = time()\n",
    "        print(f\"Tuning {name} done in {t1-t0:.1f}s. Best ROC-AUC: {rsearch.best_score_:.4f}\")\n",
    "        best_pipe = rsearch.best_estimator_\n",
    "        tuned_estimators[name] = best_pipe\n",
    "        random_search_results.append({\"model\": name, \"best_score\": rsearch.best_score_, \"best_params\": rsearch.best_params_})\n",
    "    except Exception as e:\n",
    "        print(\"Tuning failed for\", name, \":\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aea977",
   "metadata": {
    "id": "qqC2AOIzyFBX"
   },
   "source": [
    "# Build a stacking ensemble from the tuned estimators (if available)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b5867",
   "metadata": {
    "id": "qqC2AOIzyFBX"
   },
   "source": [
    "# Stacking Ensemble\n",
    "Combine the tuned models into a Stacking Classifier to leverage the strengths of multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf562efc",
   "metadata": {
    "id": "_gXK-UVMx6SI",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "estimators_for_stack = []\n",
    "for name, pipe in tuned_estimators.items():\n",
    "    # use the clf step inside the pipeline as estimator\n",
    "    estimators_for_stack.append((name, pipe.named_steps[\"clf\"]))\n",
    "\n",
    "# fallback: if no tuned estimators, use top raw candidates\n",
    "if not estimators_for_stack:\n",
    "    for nm in top_models:\n",
    "        estimators_for_stack.append((nm, candidates[nm]))\n",
    "\n",
    "# define final meta-estimator\n",
    "meta_clf = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "\n",
    "# stacking requires base estimators that accept fit(X,y) numpy arrays\n",
    "# We'll build a final pipeline: preprocessor -> SMOTE -> StackingClassifier\n",
    "base_estimators = [(n, e) for n, e in estimators_for_stack]\n",
    "stack_clf = StackingClassifier(estimators=base_estimators, final_estimator=meta_clf, n_jobs=N_JOBS, passthrough=False)\n",
    "\n",
    "final_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"clf\", stack_clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc73888",
   "metadata": {
    "id": "cl5m-vzwyUBX"
   },
   "source": [
    "# Evaluate final stacked pipeline with CV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f80e27",
   "metadata": {
    "id": "cl5m-vzwyUBX"
   },
   "source": [
    "# Final Model Evaluation\n",
    "Evaluate the final Stacking Ensemble on the held-out test set to get an unbiased estimate of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0425c83b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "Ed_T5aGWyWUS",
    "lines_to_next_cell": 2,
    "outputId": "59681baa-db8b-4f8a-ec6a-b651de118a96"
   },
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating final stacking pipeline with cross-validation...\")\n",
    "stack_scores = cross_val_score(final_pipe, X_train, y_train, cv=cv, scoring=\"roc_auc\", n_jobs=N_JOBS)\n",
    "print(\"Stacking CV ROC-AUC:\", np.mean(stack_scores), np.std(stack_scores))\n",
    "\n",
    "# Fit stack on whole training set\n",
    "print(\"Fitting final pipeline on full training set...\")\n",
    "final_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38635391",
   "metadata": {
    "id": "vB3C2m3qzMp8"
   },
   "source": [
    "# Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1e43eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8AvxOkGCybSM",
    "lines_to_next_cell": 2,
    "outputId": "0a388470-42b2-4233-d451-503cdca73ae6"
   },
   "outputs": [],
   "source": [
    "y_pred = final_pipe.predict(X_test)\n",
    "y_proba = final_pipe.predict_proba(X_test)[:, 1] if hasattr(final_pipe, \"predict_proba\") else None\n",
    "\n",
    "test_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "    \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "    \"f1\": f1_score(y_test, y_pred, zero_division=0),\n",
    "    \"roc_auc\": roc_auc_score(y_test, y_proba) if y_proba is not None else None,\n",
    "    \"confusion_matrix\": confusion_matrix(y_test, y_pred),\n",
    "    \"classification_report\": classification_report(y_test, y_pred, zero_division=0)\n",
    "}\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"Final model: Stacking_ensemble\")\n",
    "print(\"==============================\")\n",
    "for k, v in test_metrics.items():\n",
    "    if k not in [\"confusion_matrix\", \"classification_report\"]:\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "print(\"\\nConfusion matrix:\\n\", test_metrics[\"confusion_matrix\"])\n",
    "print(\"\\nClassification report:\\n\", test_metrics[\"classification_report\"])\n",
    "print(\"==============================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fd954c",
   "metadata": {
    "id": "B0KZ2weG2Q_T"
   },
   "source": [
    "# Feature Importance Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a82f656",
   "metadata": {
    "id": "B0KZ2weG2Q_T"
   },
   "source": [
    "# Feature Importance\n",
    "Analyze which features are driving the model's predictions using a surrogate Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2d789e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 826
    },
    "id": "BsDXg3X82Stu",
    "outputId": "77107b7c-e2ff-45eb-d539-904b84942151"
   },
   "outputs": [],
   "source": [
    "\n",
    "# get feature names now that preprocessor is fitted\n",
    "feature_names = get_feature_names(final_pipe.named_steps[\"preprocessor\"])\n",
    "print(\"\\nFinal feature names (count):\", len(feature_names))\n",
    "\n",
    "fi = None  # so we can safely refer to it later\n",
    "try:\n",
    "    # transform X_train to preprocessed array\n",
    "    X_train_trans = final_pipe.named_steps[\"preprocessor\"].transform(X_train)\n",
    "\n",
    "    # surrogate RF to approximate feature importances\n",
    "    rf_for_importance = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=N_JOBS\n",
    "    )\n",
    "    rf_for_importance.fit(X_train_trans, y_train)\n",
    "\n",
    "    importances = rf_for_importance.feature_importances_\n",
    "    fi = pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
    "    fi = fi.sort_values(\"importance\", ascending=False).head(TOP_K_FEATURES)\n",
    "\n",
    "    print(\"\\nTop feature importances (approx, via surrogate RF):\")\n",
    "    print(fi.to_string(index=False))\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(8, max(3, 0.3 * len(fi))))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=fi)\n",
    "    plt.title(\"Surrogate RF Feature importances (top features)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Could not compute surrogate feature importances:\", e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3130a93e",
   "metadata": {
    "id": "cZVL_93g2ghp"
   },
   "source": [
    "# Save final pipeline & summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6145d99c",
   "metadata": {
    "id": "cZVL_93g2ghp"
   },
   "source": [
    "# Save Model\n",
    "Save the final trained pipeline and performance summaries to disk for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a478a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PePj02Ph2fMB",
    "lines_to_next_cell": 2,
    "outputId": "a94c095c-98e8-4f41-87df-c8bdf020deaf"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Save model pipeline\n",
    "# Inject feature importances into the pipeline object if possible\n",
    "try:\n",
    "    stacking_clf = final_pipe.named_steps['clf']\n",
    "    # Try to find a tree-based model among base estimators\n",
    "    for estimator in stacking_clf.estimators_:\n",
    "        if hasattr(estimator, 'feature_importances_'):\n",
    "            final_pipe.feature_importances_ = estimator.feature_importances_\n",
    "            print(\"Injected feature_importances_ from base estimator into final_pipe.\")\n",
    "            break\n",
    "except Exception as e:\n",
    "    print(f\"Could not inject feature importances: {e}\")\n",
    "\n",
    "joblib.dump(final_pipe, OUTPUT_MODEL_PATH)\n",
    "print(\"Saved final pipeline to:\", OUTPUT_MODEL_PATH)\n",
    "\n",
    "# Save metrics + CV summary to CSV\n",
    "summary_rows = [{\n",
    "    \"model\": \"Stacking_ensemble\",\n",
    "    \"accuracy\": test_metrics[\"accuracy\"],\n",
    "    \"precision\": test_metrics[\"precision\"],\n",
    "    \"recall\": test_metrics[\"recall\"],\n",
    "    \"f1\": test_metrics[\"f1\"],\n",
    "    \"roc_auc\": test_metrics[\"roc_auc\"]\n",
    "}]\n",
    "\n",
    "for r in model_scores:\n",
    "    summary_rows.append({\n",
    "        \"model\": r[\"model\"],\n",
    "        \"mean_roc_auc\": r[\"mean_roc_auc\"],\n",
    "        \"std_roc_auc\": r[\"std_roc_auc\"]\n",
    "    })\n",
    "\n",
    "summary = pd.DataFrame(summary_rows)\n",
    "summary.to_csv(OUTPUT_SUMMARY_CSV, index=False)\n",
    "print(\"Saved model comparison to:\", OUTPUT_SUMMARY_CSV)\n",
    "\n",
    "# Save text summary\n",
    "with open(OUTPUT_TEXT_SUMMARY, \"w\") as f:\n",
    "    f.write(\"Final selected model: Stacking_ensemble\\n\\n\")\n",
    "    f.write(\"Test metrics:\\n\")\n",
    "    for k, v in test_metrics.items():\n",
    "        f.write(f\"{k}:\\n{v}\\n\\n\")\n",
    "\n",
    "    f.write(\"Top features (approx):\\n\")\n",
    "    if fi is not None:\n",
    "        f.write(fi.to_string(index=False))\n",
    "    else:\n",
    "        f.write(\"n/a\\n\")\n",
    "\n",
    "print(\"Saved text summary to:\", OUTPUT_TEXT_SUMMARY)\n",
    "\n",
    "print(\"\\nDone. Model pipeline saved at (local path):\", OUTPUT_MODEL_PATH)\n",
    "print(\"You can use joblib.load(OUTPUT_MODEL_PATH) to load it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72921285",
   "metadata": {},
   "source": [
    "## Prediction on New Data\n",
    "Demonstrate how to load the saved model and make predictions on new, unseen customer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e4255",
   "metadata": {
    "id": "0oeJwb1NLnNY"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load pipeline\n",
    "final_pipe = joblib.load(\"model.pkl\")\n",
    "\n",
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"usage_intensity\"] = df[\"monthly_usage_hours\"] / (df[\"num_transactions\"] + 1e-6)\n",
    "    df[\"complaint_ratio\"] = df[\"complaints\"] / (df[\"num_transactions\"] + 1e-6)\n",
    "    df[\"log_monthly_usage\"] = np.log1p(df[\"monthly_usage_hours\"])\n",
    "    df[\"log_num_transactions\"] = np.log1p(df[\"num_transactions\"])\n",
    "\n",
    "    def age_bucket(a):\n",
    "        if a <= 30:\n",
    "            return \"Young\"\n",
    "        elif a <= 50:\n",
    "            return \"Adult\"\n",
    "        else:\n",
    "            return \"Senior\"\n",
    "    df[\"age_bucket\"] = df[\"age\"].apply(age_bucket)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3119ddd3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tNI5EMpHEj5C",
    "lines_to_next_cell": 2,
    "outputId": "a7e13366-71ec-417b-891a-5ed54a77e8e1"
   },
   "outputs": [],
   "source": [
    "new_customers_raw = pd.DataFrame([\n",
    "    # 1. Very loyal, heavy user, zero complaints → expected low churn\n",
    "    {\n",
    "        \"age\": 28,\n",
    "        \"gender\": \"Male\",\n",
    "        \"monthly_usage_hours\": 120,   # watches a lot\n",
    "        \"num_transactions\": 12,       # many payments/renewals\n",
    "        \"complaints\": 0,\n",
    "        \"subscription_type\": \"Premium\"\n",
    "    },\n",
    "    # 2. Low usage, many complaints, basic plan → expected high churn\n",
    "    {\n",
    "        \"age\": 35,\n",
    "        \"gender\": \"Female\",\n",
    "        \"monthly_usage_hours\": 5,\n",
    "        \"num_transactions\": 2,\n",
    "        \"complaints\": 3,\n",
    "        \"subscription_type\": \"Basic\"\n",
    "    },\n",
    "    # 3. Medium usage but very high complaint ratio → likely to churn\n",
    "    {\n",
    "        \"age\": 22,\n",
    "        \"gender\": \"Male\",\n",
    "        \"monthly_usage_hours\": 40,\n",
    "        \"num_transactions\": 3,\n",
    "        \"complaints\": 4,  # more complaints than transactions\n",
    "        \"subscription_type\": \"Standard\"\n",
    "    },\n",
    "    # 4. Older user, moderate usage, no complaints, long history → probably stable\n",
    "    {\n",
    "        \"age\": 55,\n",
    "        \"gender\": \"Female\",\n",
    "        \"monthly_usage_hours\": 60,\n",
    "        \"num_transactions\": 20,\n",
    "        \"complaints\": 0,\n",
    "        \"subscription_type\": \"Standard\"\n",
    "    },\n",
    "    # 5. New user: very low usage, low transactions, no complaints yet → ambiguous / moderate churn risk\n",
    "    {\n",
    "        \"age\": 30,\n",
    "        \"gender\": \"Other\",\n",
    "        \"monthly_usage_hours\": 8,\n",
    "        \"num_transactions\": 1,\n",
    "        \"complaints\": 0,\n",
    "        \"subscription_type\": \"Basic\"\n",
    "    },\n",
    "])\n",
    "\n",
    "# Add engineered features\n",
    "new_customers = add_features(new_customers_raw)\n",
    "\n",
    "# Predict\n",
    "probs = final_pipe.predict_proba(new_customers)[:, 1]  # probability of churn\n",
    "preds = final_pipe.predict(new_customers)\n",
    "\n",
    "for i, (p, prob) in enumerate(zip(preds, probs), start=1):\n",
    "    print(f\"Customer {i}: predicted_churn={p}, churn_probability={prob:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af1945e",
   "metadata": {
    "id": "2RQe8B-pEk3l"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
